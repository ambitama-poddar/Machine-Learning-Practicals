{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMQy80Rdqt/yFEdfNPBsBcG",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ambitama-poddar/Machine-Learning-Practicals/blob/main/RNN(long).ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Rbx16K0sFPve"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from keras.models import Model\n",
        "from keras.layers import LSTM, Activation, Dense, Dropout, Input, Embedding\n",
        "from keras.optimizers import RMSprop\n",
        "from keras.preprocessing.text import Tokenizer\n",
        "from keras.preprocessing import sequence\n",
        "from keras.utils import to_categorical\n",
        "from keras.callbacks import EarlyStopping\n",
        "%matplotlib inline"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "8xRt72ryFXB0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "Path = 'drive/My Drive/Colab Notebooks'"
      ],
      "metadata": {
        "id": "RVH4fkXmFYo0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data = pd.read_csv(Path+'/Sentiment140_dataset.csv',delimiter=',',encoding='latin-1',names=['polarity','id','date','query','user','text'])\n",
        "data.head(20)"
      ],
      "metadata": {
        "id": "ykH8duiVFciV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Dataset shape:\",data.shape)"
      ],
      "metadata": {
        "id": "n8M9VLxnFeII"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data['polarity'].unique()"
      ],
      "metadata": {
        "id": "vlDtb0knFgPx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data['polarity']=data['polarity'].replace(4,1)\n",
        "data.head(1600000)"
      ],
      "metadata": {
        "id": "DIbacmjGFhzv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data.describe()"
      ],
      "metadata": {
        "id": "Hyo_rsaOFjQs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "positives=data['polarity'][data.polarity==1]\n",
        "negatives=data['polarity'][data.polarity==0]\n",
        "\n",
        "print('Total length of the data is:                    {}'.format(data.shape[0]))\n",
        "print('Number of positive tagged sentences is:         {}'.format(len(positives)))\n",
        "print('Number of negative tagged sentences is:         {}'.format(len(negatives)))"
      ],
      "metadata": {
        "id": "N4FQXBDJFlU_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data.drop(['id', 'date', 'query', 'user'], axis=1, inplace=True)\n",
        "data.info()"
      ],
      "metadata": {
        "id": "6l6gzlEBFoBE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data.head(10)"
      ],
      "metadata": {
        "id": "Ix2-YpSMFplp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "(data.isnull().sum() / len(data))*100"
      ],
      "metadata": {
        "id": "2HKSLs3AFrJD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data['text'] = data['text'].astype('str')"
      ],
      "metadata": {
        "id": "r6GT75V6FtKj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.tokenize import word_tokenize\n",
        "\n",
        "nltk.download('stopwords')\n",
        "stopword=set(stopwords.words('english'))\n",
        "print(stopword)"
      ],
      "metadata": {
        "id": "-zM4I8jlFvSL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "import string\n",
        "\n",
        "urlPattern = r\"((http://)[^ ]*|(https://)[^ ]*|( www\\.)[^ ]*)\"\n",
        "userPattern = '@[^\\s]+'\n",
        "def process_tweets(tweet):\n",
        "  # Lower Casing\n",
        "  tweet = tweet.lower()\n",
        "  tweet=tweet[1:]\n",
        "  #Removing all UR1s\n",
        "  tweet = re.sub(urlPattern,'',tweet)\n",
        "  #Removing all @username.\n",
        "  tweet = re.sub(userPattern,'', tweet)\n",
        "  #Remove punctuations\n",
        "  tweet = tweet.translate(str.maketrans(\"\",\"\",string.punctuation))\n",
        "  #tokenizing words\n",
        "  tokens = word_tokenize(tweet)\n",
        "  #Removing Stop Words\n",
        "  final_tokens = [w for w in tokens if w not in stopword]\n",
        "  #reducing a word to its word stem\n",
        "  wordLemm = WordNetLemmatizer()\n",
        "  finalwords=[]\n",
        "  for w in final_tokens:\n",
        "    if len(w)>1:\n",
        "      word = wordLemm.lemmatize(w)\n",
        "      finalwords.append(word)\n",
        "  return ' '.join(finalwords)"
      ],
      "metadata": {
        "id": "zlQCfuTkFwmX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "nltk.download('punkt')\n",
        "nltk.download('wordnet')"
      ],
      "metadata": {
        "id": "an_DnHQ9Fz6V"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data['processed_tweets'] = data['text'].apply(lambda x: process_tweets(x))\n",
        "print('Text Preprocessing complete.')"
      ],
      "metadata": {
        "id": "mb5CqifsF11k"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data.head(10)"
      ],
      "metadata": {
        "id": "Qp7e8GsAF3kw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "\n",
        "X = data['processed_tweets'].values\n",
        "Y = data['polarity'].values\n",
        "print(X.shape)\n",
        "print(Y.shape)\n",
        "\n",
        "vector = TfidfVectorizer(sublinear_tf=True)\n",
        "X=vector.fit_transform(X)\n",
        "print(f'Vector fitted.')\n",
        "\n",
        "print(X.shape)\n",
        "print(Y.shape)"
      ],
      "metadata": {
        "id": "4iLcbpZDF5Sx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from keras.preprocessing.text import Tokenizer\n",
        "from keras.utils import pad_sequences\n",
        "from keras import regularizers\n",
        "\n",
        "max_words = 5000\n",
        "max_len = 200\n",
        "\n",
        "tokenizer = Tokenizer(num_words=max_words)\n",
        "tokenizer.fit_on_texts(data.processed_tweets)\n",
        "sequences = tokenizer.texts_to_sequences(data.processed_tweets)\n",
        "tweets = pad_sequences(sequences, maxlen=max_len)\n",
        "print(tweets)"
      ],
      "metadata": {
        "id": "9uxdv8WfF738"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X_train,X_test,Y_train,Y_test = train_test_split(tweets,data.polarity.values,test_size=0.2,random_state=101)"
      ],
      "metadata": {
        "id": "Fi0xl2yyF-nC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"X_train\",X_train.shape)\n",
        "print(\"Y_train\",Y_train.shape)\n",
        "print()\n",
        "print(\"X_test\",X_test.shape)\n",
        "print(\"Y_test\",Y_test.shape)"
      ],
      "metadata": {
        "id": "Mw2lhG9ZGB5B"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from keras.models import Sequential\n",
        "from keras import layers\n",
        "from keras import regularizers\n",
        "from keras import backend as K\n",
        "from keras.callbacks import ModelCheckpoint\n",
        "model=Sequential()\n",
        "model.add(layers.Embedding(max_words, 128))\n",
        "model.add(layers.LSTM(64, dropout=0.5))\n",
        "model.add(layers.Dense(16, activation='relu'))\n",
        "model.add(layers.Dense(8, activation='relu'))\n",
        "model.add(layers.Dense(1, activation='sigmoid'))\n",
        "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "model.summary()"
      ],
      "metadata": {
        "id": "qsBoHupCGEBn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.fit(X_train,Y_train,batch_size=128,epochs=10,\n",
        "          validation_split=0.2,callbacks=[EarlyStopping(monitor='val_loss',min_delta=0.0001)])"
      ],
      "metadata": {
        "id": "gRMY0sYxGHlM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "accr = model.evaluate(X_test,Y_test)"
      ],
      "metadata": {
        "id": "fdSVl0TeGIVy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print('Test set\\n  Loss: {:0.3f}\\n  Accuracy: {:0.3f}'.format(accr[0],accr[1]))"
      ],
      "metadata": {
        "id": "44S44SMIGMdC"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}